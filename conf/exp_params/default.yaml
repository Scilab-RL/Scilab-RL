# @package _global_

# The name of the OpenAI Gym environment that you want to train on.
#env: 'Blocks-o1-gripper_random-v1'
#env: 'AntReacher-v1'
#env: 'ButtonUnlock-o1-v1'
env: 'FetchReach-v1'
#env: 'AntMaze-v0'
# Currently supported envs:
# 'FetchPush-v1',
# 'FetchSlide-v1',
# 'FetchPickAndPlace-v1',
# 'FetchReach-v1',

# 'HandManipulateBlock-v0',
# 'Hook-o1-v1',
# 'ButtonUnlock-o2-v1',
# 'ButtonUnlock-o1-v1',

# 'AntReacher-v1',
# 'Ant4Rooms-v1',
# 'AntMaze-v0',
# 'AntPush-v0',
# 'AntFall-v0',

# 'BlockStackMujocoEnv-gripper_random-o0-v1',
# 'BlockStackMujocoEnv-gripper_random-o2-v1',
# 'BlockStackMujocoEnv-gripper_above-o1-v1',
# 'BlockStackMujocoEnv-gripper_none-o1-v1',

seed: 0

# the path to where logs and policy pickles should go.
base_logdir: 'data'

# The pretrained policy file to start with to avoid learning from scratch again. Useful for interrupting and restoring training sessions.
restore_policy: null

# Whether to use Hinsight Experience Replay if the algorithm supports it (if it is an offline algorithm)
using_her: True

# The number of training steps after which to evaluate the policy.
eval_after_n_steps: 2000

# The max. number of training epochs to run. One epoch consists of 'eval_after_n_steps' actions.
n_epochs: 60

# The number of testing rollouts.
n_test_rollouts: 10

# Max. number of tries for this training config.
max_try_idx: 399

# Index for first try.
try_start_idx: 100

# The n last epochs over which to average for determining early stopping condition.
early_stop_last_n: 3

# The early stopping threshold.
early_stop_threshold: 0.9

# The data column on which early stopping is based.
early_stop_data_column: 'test/success_rate'

# A command line comment that will be integrated in the folder where the results
# are stored. Useful for debugging and addressing temporary changes to the code..
info: ''

# The number of steps after which to save the model. 0 to never save, i.e., to only save the best and last model.
save_model_freq: 0

# The render_args specify how and when to render during training (first sublist) and testing (second sublist).
# 'record' is for video, 'display' for direct visualization, 'none' for not rendering at all.
# The numbers determine the number of epochs after which we render the training/testing.
# Example: [['display',1],['record',10]] means that we display every training and record every 10th testing run.
render_args: [['none',1],['none',1]]

hydra:
  run:
    dir: ${base_logdir}/${git_label:}/${env}/${now:%H-%M-%S}
  sweep:
    dir: ${base_logdir}/${git_label:}/${env}/${now:%H-%M-%S}
    subdir: ${hydra.job.num}
